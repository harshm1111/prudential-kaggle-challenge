{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(data_frame):\n",
    "#     for column in data_frame:\n",
    "#         if data_frame[column].count() < len(data_frame.index)/2:\n",
    "#             del data_frame[column]\n",
    "    \n",
    "    def calc_coeff(attr_array, target, coeff):\n",
    "        len_target = len(target)\n",
    "        c_sum = np.sum((attr_array.dot(coeff) - target) ** 2) / (2 * len_target)\n",
    "        return c_sum\n",
    "    \n",
    "    def grad_descent(attr_array, target, coeff, alpha, cycles):\n",
    "    #coeff_list = [0] * cycles\n",
    "        len_target = len(target)\n",
    "\n",
    "        for i in range(cycles):\n",
    "            # Predicted value\n",
    "            pred_val = attr_array.dot(coeff)\n",
    "            # Difference between prediction and Actual target\n",
    "            diff = pred_val - target\n",
    "            # Calculating gradient\n",
    "            grad = attr_array.T.dot(diff) / len_target\n",
    "            # Replace coefficient values with the new ones\n",
    "            coeff = coeff - (grad * alpha)\n",
    "            final_coeff = calc_coeff(attr_array, target, coeff)\n",
    "            #coeff_list[i] = final_coeff\n",
    "        return coeff\n",
    "    df1 = data_frame[['Ht','Wt','BMI']]\n",
    "    df1 = df1.dropna()\n",
    "    for col in df1:\n",
    "        df1[col] = np.around(df1[col], decimals = 2)\n",
    "    Ht_len = len(df1['Ht'].values)\n",
    "    Wt_len = len(df1['Wt'].values)\n",
    "    bmi_len = len(df1['BMI'].values)\n",
    "    Ht_n_ones = np.ones(Ht_len)\n",
    "    Wt_n_ones = np.ones(Wt_len)\n",
    "    bmi_n_ones = np.ones(bmi_len)\n",
    "    bmi_attr_array = np.array([Ht_n_ones, df1['Ht'].values, df1['Wt'].values]).T\n",
    "    Wt_attr_array = np.array([Ht_n_ones, df1['Ht'].values, df1['BMI'].values]).T\n",
    "    Ht_attr_array = np.array([bmi_n_ones, df1['BMI'].values, df1['Wt'].values]).T\n",
    "    coeff = np.array([0, 0, 0])\n",
    "    bmi_target = np.array(df1['BMI'].values)\n",
    "    Ht_target = np.array(df1['Ht'].values)\n",
    "    Wt_target = np.array(df1['Wt'].values)\n",
    "    alpha = 0.01\n",
    "    \n",
    "    Ht_grad_coef = grad_descent(Ht_attr_array, Ht_target, coeff, alpha, 100000)\n",
    "    Wt_grad_coef = grad_descent(Wt_attr_array, Wt_target, coeff, alpha, 100000)\n",
    "    bmi_grad_coef = grad_descent(Ht_attr_array, Ht_target, coeff, alpha, 100000)       \n",
    "    keywords_data_frame = data_frame.columns[data_frame.columns.str.startswith('Medical_History_', 'Product_Info_')]\n",
    "    keywords_data_frame = [x for x in keywords_data_frame if not x.startswith(('Medical_History_1','Medical_History_2','Medical_History_15','Medical_History_24'))]\n",
    "    for i in keywords_data_frame:\n",
    "        data_frame[i] = data_frame[i].fillna((data_frame[i].mode()))\n",
    "    Emp_keywords_data_frame = data_frame.columns[data_frame.columns.str.startswith('Employment_Info_','InsuredInfo_')]\n",
    "    Emp_keywords_data_frame = [x for x in keywords_data_frame if not x.startswith(('Employment_Info_1','Employment_Info_4','Employment_Info_6','InsuredInfo_3'))]\n",
    "    for i in Emp_keywords_data_frame:\n",
    "        data_frame[i] = data_frame[i].fillna((data_frame[i].mode()))\n",
    "    data_frame['Insurance_History_1'] = data_frame['Insurance_History_1'].fillna(data_frame['Insurance_History_1'].mode())\n",
    "    data_frame['Insurance_History_2'] = data_frame['Insurance_History_2'].fillna(data_frame['Insurance_History_2'].mode())\n",
    "    data_frame['Family_Hist_1'] = data_frame['Family_Hist_1'].fillna(data_frame['Family_Hist_1'].mode())\n",
    "    data_frame['Wt'] = np.around(data_frame['Wt'], decimals = 2)\n",
    "    data_frame['Ht'] = np.around(data_frame['Ht'], decimals = 2)\n",
    "    data_frame['BMI'] = np.around(data_frame['BMI'], decimals = 2)\n",
    "    BMI_list = data_frame[(data_frame.Wt.notnull()) & (data_frame.Ht.notnull()) & (data_frame.BMI.isnull())].index.tolist()\n",
    "    for i in BMI_list:\n",
    "#         data_frame['BMI'].iloc[i] = (data_frame['Ht'].iloc[i] * -1.04556593) + (data_frame['Wt'].iloc[i] * 1.70351364) + 0.71074665089157529\n",
    "#         data_frame['BMI'].iloc[i] = (data_frame['Ht'].iloc[i] * 1.6662764) + (data_frame['Wt'].iloc[i] * 0.07754808) + 0.23512214\n",
    "#         data_frame['BMI'].iloc[i] = (data_frame['Ht'].iloc[i] * -0.8746338) + (data_frame['Wt'].iloc[i] * 1.59684629) + 0.62073836\n",
    "        data_frame['BMI'].iloc[i] = (data_frame['Ht'].iloc[i] * bmi_grad_coef[1]) + (data_frame['Wt'].iloc[i] * bmi_grad_coef[2]) + bmi_grad_coef[0]\n",
    "    Wt_list = data_frame[(data_frame.Ht.notnull()) & (data_frame.BMI.notnull()) & (data_frame.Wt.isnull())].index.tolist()\n",
    "    for i in Wt_list:\n",
    "#         data_frame['Wt'].iloc[i] = (data_frame['Ht'].iloc[i] * 0.61556981) + (data_frame['BMI'].iloc[i] * 0.57665336) - 0.41360434245416683\n",
    "#         data_frame['Wt'].iloc[i] = (data_frame['Ht'].iloc[i] * 0.10120819) + (data_frame['BMI'].iloc[i] * 0.07351834) + 0.1379455\n",
    "#         data_frame['Wt'].iloc[i] = (data_frame['Ht'].iloc[i] * 0.59756114) + (data_frame['BMI'].iloc[i] * 0.57567602) - 0.40025912\n",
    "         data_frame['Wt'].iloc[i] = (data_frame['Ht'].iloc[i] * Wt_grad_coef[1]) + (data_frame['BMI'].iloc[i] * Wt_grad_coef[2]) + Wt_grad_coef[0]\n",
    "    Ht_list = data_frame[(data_frame.Wt.notnull()) & (data_frame.BMI.notnull()) & (data_frame.Ht.isnull())].index.tolist()\n",
    "    for i in Ht_list:\n",
    "#         data_frame['Ht'].iloc[i] = (data_frame['Wt'].iloc[i] * 1.55752046) + (data_frame['BMI'].iloc[i] * -0.8955237) + 0.67214516469349728\n",
    "#         data_frame['Ht'].iloc[i] = (data_frame['BMI'].iloc[i] * 0.18446449) + (data_frame['Wt'].iloc[i] * 0.11822116) + 0.39463004\n",
    "#         data_frame['Ht'].iloc[i] = (data_frame['Wt'].iloc[i] * 1.2330216) + (data_frame['BMI'].iloc[i] * -0.67066157) + 0.66137008\n",
    "        data_frame['Ht'].iloc[i] = (data_frame['BMI'].iloc[i] * Ht_grad_coef[1]) + (data_frame['BMI'].iloc[i] * Ht_grad_coef[2]) + Ht_grad_coef[0]\n",
    "    data_frame['Ins_Age'] = np.around(data_frame['Ins_Age'], decimals = 2)\n",
    "    data_frame['Ins_Age'] = data_frame['Ins_Age'].fillna(data_frame['Ins_Age'].mean())\n",
    "#     data_frame['Ht'] = np.around(data_frame['Ht'], decimals = 2)\n",
    "    data_frame['Ht'] = data_frame['Ht'].fillna(data_frame['Ht'].mean())\n",
    "#     data_frame['Wt'] = np.around(data_frame['Wt'], decimals = 2)\n",
    "    data_frame['Wt'] = data_frame['Wt'].fillna(data_frame['Wt'].mean())\n",
    "    data_frame['Product_Info_2'] = pd.factorize(data_frame['Product_Info_2'])[0]\n",
    "#     data_frame = data_frame.dropna(subset=['BMI'], how='all')\n",
    "#     data_frame['BMI'] = data_frame['BMI'].fillna((-1.04556593 * data_frame['Ht']) + (1.70351364 * data_frame['Wt']) + 0.71074665089157529)\n",
    "#     data_frame['BMI'] = data_frame['BMI'].fillna((data_frame['Ht'].iloc[i] * 1.6662764) + (data_frame['Wt'].iloc[i] * 0.07754808) + 0.23512214)\n",
    "    data_frame['BMI'] = data_frame['BMI'].fillna((data_frame['Ht'].iloc[i] * -0.8746338) + (data_frame['Wt'].iloc[i] * 1.59684629) + 0.62073836)\n",
    "    data_frame['BMI_Age'] = data_frame['BMI'] * data_frame['Ins_Age']\n",
    "#     data_frame['BMI_zscore'] = (data_frame['BMI'] - data_frame['BMI'].mean())/data_frame['BMI'].std(ddof=0)\n",
    "#     data_frame = data_frame.drop(['BMI'], axis = 1)\n",
    "    med_keyword_columns = data_frame.columns[data_frame.columns.str.startswith('Medical_Keyword_')]\n",
    "    data_frame['Med_Keywords_Count'] = data_frame[med_keyword_columns].sum(axis=1)\n",
    "    data_frame['prop_BMI_Wt'] = ((data_frame['BMI'] * data_frame['Wt'])** 2)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harshmangal/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"training.csv\")\n",
    "df_all = train\n",
    "df = clean_data(df_all)\n",
    "df.fillna(-1, inplace=True)\n",
    "cols = list(df.columns.values)\n",
    "cols.pop(cols.index('Response'))\n",
    "df = df[cols+['Response']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"cleaned_training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"testing.csv\")\n",
    "main_solution = pd.DataFrame(test_data['Id'])\n",
    "test_data = test_data.drop(['Id'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harshmangal/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "test_data1 = clean_data(test_data)\n",
    "test_data1.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data1.to_csv('cleaned_testing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The Attribute class which includes the column number and the value for the column number\n",
    "#This class contains the value of the attribute which is used to divide the dataset at each node of the tree\n",
    "class attribute:\n",
    "    def __init__(self,value,index):\n",
    "        self.value = value\n",
    "        self.index = index      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function compares an attribute x with another attribute to see \n",
    "#which side of the tree the given attribute x should belong to\n",
    "def compare_attribute(x,attribute):   \n",
    "    compare_to = x[attribute.index]\n",
    "    if ((type(compare_to)== type(7))or(type(compare_to)==type(7.7))):\n",
    "        return (compare_to >= attribute.value)\n",
    "    else:\n",
    "        return (compare_to == attribute.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function divides the data based on the attribute\n",
    "def divide_data(data,attribute):\n",
    "    left_data = []\n",
    "    right_data = []\n",
    "    for x in data:\n",
    "        if compare_attribute(x,attribute): \n",
    "            right_data.append(x)\n",
    "        else:\n",
    "            left_data.append(x)\n",
    "    return left_data,right_data  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculates gini index of given data\n",
    "def gini(data):\n",
    "    temp = []\n",
    "    count_temp = []\n",
    "    for x in data:\n",
    "        if (x[len(x)-1] not in temp):\n",
    "            temp.append(x[len(x)-1])\n",
    "            count_temp.append(0)\n",
    "        count_temp[temp.index(x[len(x)-1])] += 1\n",
    "    out = 1;\n",
    "    for i in range(0,len(temp)):\n",
    "        curr_probability = count_temp[i]/len(data)\n",
    "        out = out - curr_probability**2\n",
    "    return out\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculates the reduction in gini index after the division has been performed\n",
    "def gini_reduction(initial_gini,left_data,right_data):\n",
    "    probability_left = len(left_data)/(len(left_data)+len(right_data))\n",
    "    probability_right = 1-probability_left\n",
    "    out = initial_gini-probability_left*gini(left_data)-probability_right*gini(right_data)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Finds the best division attribute for the given data based on gini index\n",
    "def division_attribute(data):\n",
    "    output_attribute = None\n",
    "    max_gini_reduction = 0;\n",
    "    initial_gini = gini(data)\n",
    "    for i in range(0,len(data[0])-1):\n",
    "        column_unique_values = []\n",
    "        for x in data:\n",
    "            if (x[i] not in column_unique_values):\n",
    "                column_unique_values.append(x[i])\n",
    "        for y in column_unique_values:\n",
    "            curr_attribute = attribute(y,i)\n",
    "            left_data,right_data = divide_data(data,curr_attribute)\n",
    "            curr_gini_reduction = gini_reduction(initial_gini,left_data,right_data)\n",
    "            if ((curr_gini_reduction >= max_gini_reduction)and(curr_gini_reduction != 0)):\n",
    "                max_gini_reduction = curr_gini_reduction\n",
    "                output_attribute = curr_attribute\n",
    "    return output_attribute\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Counts instances of all classes in the given dataset\n",
    "def count_instances(data):\n",
    "    out = {}\n",
    "    for x in data:\n",
    "        temp = x[len(x)-1]\n",
    "        if temp not in out:\n",
    "            out[temp] = 1\n",
    "        else:\n",
    "            out[temp] += 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The Node Datastructure implementation for the Decision Tree\n",
    "class Node:\n",
    "    def __init__(self, left_branch, right_branch, attribute, data):\n",
    "        self.left_branch = left_branch\n",
    "        self.right_branch = right_branch\n",
    "        self.attribute = attribute\n",
    "        if (attribute == None): self.content = count_instances(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Construction of a decision tree from given data\n",
    "def make_decision_tree(data):\n",
    "    if (len(data) <= 30): return Node(None,None,None,data)\n",
    "    attribute = division_attribute(data)\n",
    "    if (attribute == None): return Node(None,None,None,data)\n",
    "    left_data, right_data = divide_data(data, attribute)\n",
    "    left_branch = make_decision_tree(left_data)\n",
    "    right_branch = make_decision_tree(right_data)\n",
    "    return Node(left_branch, right_branch, attribute,data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predicts the class of sample from the tree with root node\n",
    "def predict(sample, node):\n",
    "    if (node.attribute == None):\n",
    "        return node.content\n",
    "    if compare_attribute(sample,node.attribute):\n",
    "        return predict(sample, node.right_branch)\n",
    "    else:\n",
    "        return predict(sample, node.left_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Makes a dataset based on the weights\n",
    "import random\n",
    "def make_data_set(training_data,weights):\n",
    "    curr_set = []\n",
    "    for i in range(0,len(training_data)):\n",
    "        random_number  = random.uniform(0,1)\n",
    "        sum = 1/len(training_data)\n",
    "        for j in range(0,len(weights)):\n",
    "            if (random_number < sum):\n",
    "                curr_set.append(training_data[j])\n",
    "                break\n",
    "            sum +=1/len(training_data)    \n",
    "    return curr_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Merges the prediction results of multiple trees\n",
    "def merge(dicts):\n",
    "    out = {}\n",
    "    for data in dicts:\n",
    "        for x in data.keys():\n",
    "            if x not in out:\n",
    "                out[x] = 0\n",
    "            out[x] += data[x] \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Returns a class based on multiple prediction dictionaries\n",
    "def predict_result(dict_predictions):    \n",
    "    curr_dict = dict_predictions\n",
    "    out = 0 \n",
    "    num = 0\n",
    "    for x in curr_dict.keys():\n",
    "        if (curr_dict[x]>num):\n",
    "            out = int(x)\n",
    "            num = curr_dict[x]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Assigns votes to the classifier results\n",
    "def process(dict,vote):\n",
    "    out  = {}\n",
    "    for x in dict.keys():\n",
    "        out[x]= dict[x]*vote\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_label_weights(training_data):\n",
    "#     a = count_instances(training_data)\n",
    "#     for x in a.keys():\n",
    "#         a[x] = len(training_data)/a[x]\n",
    "#     return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def multiply_dicts(a,b):\n",
    "#     out = {}\n",
    "#     for x in a.keys():\n",
    "#         out[x] = a[x]*b[x]\n",
    "#     return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculates gini index of the predictions results dictionary\n",
    "def gini_dict(dict_):\n",
    "    temp = dict_\n",
    "    sum_temp = 0\n",
    "    for x in temp.keys():\n",
    "        sum_temp+=temp[x]\n",
    "    out  = 1\n",
    "    for x in temp.keys():\n",
    "        temp[x] = temp[x]/sum_temp\n",
    "        out = out - temp[x]**2\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Returns the average of all classes in the given dataset\n",
    "def predict_average(dict_predictions):    \n",
    "    curr_dict = dict_predictions\n",
    "    out = 0 \n",
    "    num = 0            \n",
    "    for x in curr_dict.keys():\n",
    "            out = out+x*curr_dict[x]\n",
    "            num = num+curr_dict[x]\n",
    "    out = int(round((out/num-1)*8/7,1))\n",
    "    if (out>8): out = 8\n",
    "    if (out<1): out = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def check_and_predict_average(dict_predictions):    \n",
    "#     curr_dict = dict_predictions\n",
    "#     out = 0 \n",
    "#     num = 0\n",
    "#     for x in curr_dict.keys():\n",
    "#         c = 0\n",
    "#         for y in curr_dict.keys():\n",
    "#             if((x!=y)and(curr_dict[x]<2*curr_dict[y])): c = 1\n",
    "#         if (c==0): return int(x)        \n",
    "                \n",
    "#     for x in curr_dict.keys():\n",
    "#             out = out+x*curr_dict[x]\n",
    "#             num = num+curr_dict[x]\n",
    "#     #print(\"Out = %s and Num = %s\" %(out,num))        \n",
    "#     out = int(round((out/num-1)*8/7,1))\n",
    "#     if (out>8): out = 8\n",
    "#     if (out<1): out = 1\n",
    "#     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Reading from the file\n",
    "fo = open(\"cleaned_training.csv\", \"r\")\n",
    "\n",
    "a = fo.readline()\n",
    "a = a.strip().split(',')\n",
    "a = a[2:]\n",
    "Column_names = a\n",
    "training_data = []\n",
    "for i in range(0,20000):\n",
    "    temp = fo.readline()\n",
    "    temp = temp.strip().split(',')\n",
    "    temp = temp[2:]\n",
    "    temp = [round(float(a),1) for a in temp]\n",
    "    training_data.append(temp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Assigning Weights for AdaBoost\n",
    "weights = [1/len(training_data)]*len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Classfier number: 0\n",
      "Tree Constrcuted\n",
      "Error= 0.08667346938775337\n",
      "Vote= 1.1774728151244385\n",
      "Sum of weights = 0.807413266534544\n",
      "Starting Classfier number: 1\n",
      "Tree Constrcuted\n",
      "Error= 0.12481582322314887\n",
      "Vote= 0.9737975577038988\n",
      "Sum of weights = 0.9598207912058743\n",
      "Starting Classfier number: 2\n",
      "Tree Constrcuted\n",
      "Error= 0.17406871281750372\n",
      "Vote= 0.7785307300593732\n",
      "Sum of weights = 1.0650226199945867\n",
      "Starting Classfier number: 3\n",
      "Tree Constrcuted\n",
      "Error= 0.22921591591838303\n",
      "Vote= 0.606371932073437\n",
      "Sum of weights = 1.1121937468583845\n",
      "Starting Classfier number: 4\n",
      "Tree Constrcuted\n",
      "Error= 0.2821567527468852\n",
      "Vote= 0.4668942246574976\n",
      "Sum of weights = 1.1230695783517335\n",
      "Starting Classfier number: 5\n",
      "Tree Constrcuted\n",
      "Error= 0.32561854040638305\n",
      "Vote= 0.3640346699083365\n",
      "Sum of weights = 1.115929353668898\n",
      "Starting Classfier number: 6\n",
      "Tree Constrcuted\n",
      "Error= 0.3638404023743934\n",
      "Vote= 0.2793670775255514\n",
      "Sum of weights = 1.1004581577058221\n",
      "Starting Classfier number: 7\n",
      "Tree Constrcuted\n",
      "Error= 0.3968309559373139\n",
      "Vote= 0.20934355476470914\n",
      "Sum of weights = 1.0820049560565512\n",
      "Starting Classfier number: 8\n",
      "Tree Constrcuted\n",
      "Error= 0.4215860587051976\n",
      "Vote= 0.15813292233387155\n",
      "Sum of weights = 1.0652115501766624\n",
      "Starting Classfier number: 9\n",
      "Tree Constrcuted\n",
      "Error= 0.44518456630309905\n",
      "Vote= 0.11007327731257935\n",
      "Sum of weights = 1.0476363601820415\n",
      "Starting Classfier number: 10\n",
      "Tree Constrcuted\n",
      "Error= 0.4563174365403022\n",
      "Vote= 0.08758842669894863\n",
      "Sum of weights = 1.0386950662444254\n",
      "Starting Classfier number: 11\n",
      "Tree Constrcuted\n",
      "Error= 0.46725907690771185\n",
      "Vote= 0.06557568030788412\n",
      "Sum of weights = 1.0294919475318818\n",
      "Starting Classfier number: 12\n",
      "Tree Constrcuted\n",
      "Error= 0.47179729239360646\n",
      "Vote= 0.05646534893972417\n",
      "Sum of weights = 1.0255372800026301\n",
      "Starting Classfier number: 13\n",
      "Tree Constrcuted\n",
      "Error= 0.47813153887479953\n",
      "Vote= 0.043764842690437136\n",
      "Sum of weights = 1.020010270104884\n",
      "Starting Classfier number: 14\n",
      "Tree Constrcuted\n",
      "Error= 0.478851190686831\n",
      "Vote= 0.04232287046682435\n",
      "Sum of weights = 1.0193801827468532\n",
      "Starting Classfier number: 15\n",
      "Tree Constrcuted\n",
      "Error= 0.47691329688148976\n",
      "Vote= 0.04620626192181402\n",
      "Sum of weights = 1.0211823319760664\n",
      "Starting Classfier number: 16\n",
      "Tree Constrcuted\n",
      "Error= 0.4874817513224129\n",
      "Vote= 0.025041730500976928\n",
      "Sum of weights = 1.0116534557049193\n",
      "Starting Classfier number: 17\n",
      "Tree Constrcuted\n",
      "Error= 0.487286374423711\n",
      "Vote= 0.02543273323427268\n",
      "Sum of weights = 1.011843081220645\n",
      "Starting Classfier number: 18\n",
      "Tree Constrcuted\n",
      "Error= 0.4905512957959444\n",
      "Vote= 0.01889965838761211\n",
      "Sum of weights = 1.0088353681052218\n",
      "Starting Classfier number: 19\n",
      "Tree Constrcuted\n",
      "Error= 0.4919241367931052\n",
      "Vote= 0.01615313117847377\n",
      "Sum of weights = 1.007570653973938\n",
      "Starting Classfier number: 20\n",
      "Tree Constrcuted\n",
      "Error= 0.48773715969246123\n",
      "Vote= 0.024530599863201996\n",
      "Sum of weights = 1.011430347159346\n",
      "Starting Classfier number: 21\n",
      "Tree Constrcuted\n",
      "Error= 0.48314612787289285\n",
      "Vote= 0.033720519345900285\n",
      "Sum of weights = 1.0155868073272365\n",
      "Starting Classfier number: 22\n",
      "Tree Constrcuted\n",
      "Error= 0.49185093435885113\n",
      "Vote= 0.016299574598227444\n",
      "Sum of weights = 1.0076339921809971\n",
      "Starting Classfier number: 23\n",
      "Tree Constrcuted\n",
      "Error= 0.48982797779278525\n",
      "Vote= 0.020346851776947544\n",
      "Sum of weights = 1.0095009220678421\n",
      "Starting Classfier number: 24\n",
      "Tree Constrcuted\n",
      "Error= 0.49445640755874876\n",
      "Vote= 0.011087639215884104\n",
      "Sum of weights = 1.0052111398225683\n",
      "Starting Classfier number: 25\n",
      "Tree Constrcuted\n",
      "Error= 0.4990303940088703\n",
      "Vote= 0.001939214413094965\n",
      "Sum of weights = 1.0009170676943588\n",
      "Starting Classfier number: 26\n",
      "Tree Constrcuted\n",
      "Error= 0.4950679865399324\n",
      "Vote= 0.009864346858887497\n",
      "Sum of weights = 1.0046416558407203\n",
      "Starting Classfier number: 27\n",
      "Tree Constrcuted\n",
      "Error= 0.49689067313039453\n",
      "Vote= 0.006218733902946909\n",
      "Sum of weights = 1.0029311033014479\n",
      "Starting Classfier number: 28\n",
      "Tree Constrcuted\n",
      "Error= 0.49820575289171554\n",
      "Vote= 0.0035885096200490127\n",
      "Sum of weights = 1.0016952689231908\n",
      "Starting Classfier number: 29\n",
      "Tree Constrcuted\n",
      "Error= 0.4967549963456541\n",
      "Vote= 0.006490098431118796\n",
      "Sum of weights = 1.0030597359020803\n",
      "Starting Classfier number: 30\n",
      "Tree Constrcuted\n",
      "Error= 0.5051024385601371\n",
      "Vote= -0.010205231386068636\n",
      "Sum of weights = 0.9951383473946317\n",
      "Starting Classfier number: 31\n",
      "Tree Constrcuted\n",
      "Error= 0.5055760535749404\n",
      "Vote= -0.011152569511690852\n",
      "Sum of weights = 0.9946913874616041\n",
      "Starting Classfier number: 32\n",
      "Tree Constrcuted\n",
      "Error= 0.5015806125093312\n",
      "Vote= -0.003161235549128114\n",
      "Sum of weights = 0.9985054213931593\n",
      "Starting Classfier number: 33\n",
      "Tree Constrcuted\n",
      "Error= 0.501789295507711\n",
      "Vote= -0.003578606291725743\n",
      "Sum of weights = 0.9983096952362586\n",
      "Starting Classfier number: 34\n",
      "Tree Constrcuted\n",
      "Error= 0.49730592510288796\n",
      "Vote= 0.005388201938338136\n",
      "Sum of weights = 1.0025305762106076\n",
      "Starting Classfier number: 35\n",
      "Tree Constrcuted\n",
      "Error= 0.49930144720790565\n",
      "Vote= 0.0013971064931951236\n",
      "Sum of weights = 1.0006576244115406\n",
      "Starting Classfier number: 36\n",
      "Tree Constrcuted\n",
      "Error= 0.49886086723858203\n",
      "Vote= 0.002278269464622566\n",
      "Sum of weights = 1.0010728888700597\n",
      "Starting Classfier number: 37\n",
      "Tree Constrcuted\n",
      "Error= 0.5006009214547074\n",
      "Vote= -0.001201843488073064\n",
      "Sum of weights = 0.9994330906706632\n",
      "Starting Classfier number: 38\n",
      "Tree Constrcuted\n",
      "Error= 0.4958790656880509\n",
      "Vote= 0.008242055250483053\n",
      "Sum of weights = 1.0038583036887552\n",
      "Starting Classfier number: 39\n",
      "Tree Constrcuted\n",
      "Error= 0.49676134112273723\n",
      "Vote= 0.006477408343488138\n",
      "Sum of weights = 1.0030397807895917\n",
      "Starting Classfier number: 40\n",
      "Tree Constrcuted\n",
      "Error= 0.4995993537955366\n",
      "Vote= 0.00080129258042205\n",
      "Sum of weights = 1.0003778186283252\n",
      "Starting Classfier number: 41\n",
      "Tree Constrcuted\n",
      "Error= 0.4977939695187537\n",
      "Vote= 0.004412089591634602\n",
      "Sum of weights = 1.0020747457631358\n",
      "Starting Classfier number: 42\n",
      "Tree Constrcuted\n",
      "Error= 0.5014181661029528\n",
      "Vote= -0.0028363398118321035\n",
      "Sum of weights = 0.9986600449531166\n",
      "Starting Classfier number: 43\n",
      "Tree Constrcuted\n",
      "Error= 0.5011357204443674\n",
      "Vote= -0.002271444795203975\n",
      "Sum of weights = 0.9989279765556355\n",
      "Starting Classfier number: 44\n",
      "Tree Constrcuted\n",
      "Error= 0.5010527956255829\n",
      "Vote= -0.00210559436289724\n",
      "Sum of weights = 0.9990066512015471\n",
      "Starting Classfier number: 45\n",
      "Tree Constrcuted\n",
      "Error= 0.5002758113494928\n",
      "Vote= -0.0005516227549363217\n",
      "Sum of weights = 0.9997400318261698\n",
      "Starting Classfier number: 46\n",
      "Tree Constrcuted\n",
      "Error= 0.49725294853650076\n",
      "Vote= 0.005494158208137804\n",
      "Sum of weights = 1.0025799643904805\n",
      "Starting Classfier number: 47\n",
      "Tree Constrcuted\n",
      "Error= 0.49768714686156285\n",
      "Vote= 0.00462573926962135\n",
      "Sum of weights = 1.0021732777387518\n",
      "Starting Classfier number: 48\n",
      "Tree Constrcuted\n",
      "Error= 0.49683343426464577\n",
      "Vote= 0.006333216143661842\n",
      "Sum of weights = 1.0029740833555514\n",
      "Starting Classfier number: 49\n",
      "Tree Constrcuted\n",
      "Error= 0.49757545428171474\n",
      "Vote= 0.004849129443780711\n",
      "Sum of weights = 1.002279213861562\n",
      "Starting Classfier number: 50\n",
      "Tree Constrcuted\n",
      "Error= 0.49906240019399434\n",
      "Vote= 0.0018752018099835314\n",
      "Sum of weights = 1.0008838986161168\n",
      "Starting Classfier number: 51\n",
      "Tree Constrcuted\n",
      "Error= 0.4931942616835696\n",
      "Vote= 0.013612317336161285\n",
      "Sum of weights = 1.0063638416053167\n",
      "Starting Classfier number: 52\n",
      "Tree Constrcuted\n",
      "Error= 0.4956822421526731\n",
      "Vote= 0.008635730360528912\n",
      "Sum of weights = 1.0040559813291827\n",
      "Starting Classfier number: 53\n",
      "Tree Constrcuted\n",
      "Error= 0.5005504674032495\n",
      "Vote= -0.0011009352512980223\n",
      "Sum of weights = 0.9994796180597489\n",
      "Starting Classfier number: 54\n",
      "Tree Constrcuted\n",
      "Error= 0.5011601436716088\n",
      "Vote= -0.0023202915071670956\n",
      "Sum of weights = 0.9989025589234387\n",
      "Starting Classfier number: 55\n",
      "Tree Constrcuted\n",
      "Error= 0.5008437773698585\n",
      "Vote= -0.0016875563416822855\n",
      "Sum of weights = 0.999202672840374\n",
      "Starting Classfier number: 56\n",
      "Tree Constrcuted\n",
      "Error= 0.4974439713740411\n",
      "Vote= 0.005112101783967161\n",
      "Sum of weights = 1.0024065293315727\n",
      "Starting Classfier number: 57\n",
      "Tree Constrcuted\n",
      "Error= 0.4980213475936595\n",
      "Vote= 0.00395732547035099\n",
      "Sum of weights = 1.001864728638654\n",
      "Starting Classfier number: 58\n",
      "Tree Constrcuted\n",
      "Error= 0.501516190315839\n",
      "Vote= -0.0030323899263118765\n",
      "Sum of weights = 0.9985640357196047\n",
      "Starting Classfier number: 59\n",
      "Tree Constrcuted\n",
      "Error= 0.5010550656399513\n",
      "Vote= -0.002110134411805778\n",
      "Sum of weights = 0.9990016053229263\n",
      "Starting Classfier number: 60\n",
      "Tree Constrcuted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error= 0.5009657455425891\n",
      "Vote= -0.0019314934870943718\n",
      "Sum of weights = 0.9990863437966525\n",
      "Starting Classfier number: 61\n",
      "Tree Constrcuted\n",
      "Error= 0.4969476157148107\n",
      "Vote= 0.006104844409984978\n",
      "Sum of weights = 1.0028715761646696\n",
      "Starting Classfier number: 62\n",
      "Tree Constrcuted\n",
      "Error= 0.4978357072427334\n",
      "Vote= 0.004328612549238039\n",
      "Sum of weights = 1.002039138800485\n",
      "Starting Classfier number: 63\n",
      "Tree Constrcuted\n",
      "Error= 0.4978794726075904\n",
      "Vote= 0.004241080212402229\n",
      "Sum of weights = 1.00199878784518\n",
      "Starting Classfier number: 64\n",
      "Tree Constrcuted\n",
      "Error= 0.4988405985510948\n",
      "Vote= 0.002318807053773122\n",
      "Sum of weights = 1.0010946611929368\n",
      "Starting Classfier number: 65\n",
      "Tree Constrcuted\n",
      "Error= 0.4979681852352859\n",
      "Vote= 0.004063651897336332\n",
      "Sum of weights = 1.001918014782327\n",
      "Starting Classfier number: 66\n",
      "Tree Constrcuted\n",
      "Error= 0.4980799061614328\n",
      "Vote= 0.003840206554437036\n",
      "Sum of weights = 1.0018115818281865\n",
      "Starting Classfier number: 67\n",
      "Tree Constrcuted\n",
      "Error= 0.4979802480135867\n",
      "Vote= 0.0040395259447014616\n",
      "Sum of weights = 1.0019051973904132\n",
      "Starting Classfier number: 68\n",
      "Tree Constrcuted\n",
      "Error= 0.4985466881719453\n",
      "Vote= 0.0029066318416498087\n",
      "Sum of weights = 1.0013721242899776\n",
      "Starting Classfier number: 69\n",
      "Tree Constrcuted\n",
      "Error= 0.5016117587525056\n",
      "Vote= -0.003223528670340989\n",
      "Sum of weights = 0.9984719712084431\n",
      "Starting Classfier number: 70\n",
      "Tree Constrcuted\n",
      "Error= 0.5014608750634617\n",
      "Vote= -0.002921758440926541\n",
      "Sum of weights = 0.9986161704937745\n",
      "Starting Classfier number: 71\n",
      "Tree Constrcuted\n",
      "Error= 0.5006104084278082\n",
      "Vote= -0.0012208174621163048\n",
      "Sum of weights = 0.9994223390307377\n",
      "Starting Classfier number: 72\n",
      "Tree Constrcuted\n",
      "Error= 0.5010679874663384\n",
      "Vote= -0.002135978181068489\n",
      "Sum of weights = 0.9989891280875363\n",
      "Starting Classfier number: 73\n",
      "Tree Constrcuted\n",
      "Error= 0.5009667439448418\n",
      "Vote= -0.0019334902990567567\n",
      "Sum of weights = 0.9990855286923789\n",
      "Starting Classfier number: 74\n",
      "Tree Constrcuted\n",
      "Error= 0.5010073881135793\n",
      "Vote= -0.002014778953374495\n",
      "Sum of weights = 0.9990470929346216\n",
      "Starting Classfier number: 75\n",
      "Tree Constrcuted\n",
      "Error= 0.5005412087919647\n",
      "Vote= -0.0010824180066599494\n",
      "Sum of weights = 0.9994887872419326\n",
      "Starting Classfier number: 76\n",
      "Tree Constrcuted\n",
      "Error= 0.49918709622029156\n",
      "Vote= 0.001625808991891258\n",
      "Sum of weights = 1.0007663978785124\n",
      "Starting Classfier number: 77\n",
      "Tree Constrcuted\n",
      "Error= 0.5001376061948121\n",
      "Vote= -0.00027521239657247624\n",
      "Sum of weights = 0.9998701403090475\n",
      "Starting Classfier number: 78\n",
      "Tree Constrcuted\n",
      "Error= 0.49964120179304555\n",
      "Vote= 0.0007175965370829757\n",
      "Sum of weights = 1.0003384678700642\n",
      "Starting Classfier number: 79\n",
      "Tree Constrcuted\n",
      "Error= 0.5006902502736665\n",
      "Vote= -0.0013805014243116667\n",
      "Sum of weights = 0.9993482589361912\n",
      "Starting Classfier number: 80\n",
      "Tree Constrcuted\n",
      "Error= 0.4999368332373453\n",
      "Vote= 0.0001263335259815035\n",
      "Sum of weights = 1.0000595440529256\n",
      "Starting Classfier number: 81\n",
      "Tree Constrcuted\n",
      "Error= 0.500016267464479\n",
      "Vote= -3.2534928969507456e-05\n",
      "Sum of weights = 0.9999846504007172\n",
      "Starting Classfier number: 82\n",
      "Tree Constrcuted\n",
      "Error= 0.49980641094430733\n",
      "Vote= 0.000387178130732263\n",
      "Sum of weights = 1.0001825600279566\n",
      "Starting Classfier number: 83\n",
      "Tree Constrcuted\n",
      "Error= 0.500159933647051\n",
      "Vote= -0.00031986730501111076\n",
      "Sum of weights = 0.9998491482725137\n",
      "Starting Classfier number: 84\n",
      "Tree Constrcuted\n",
      "Error= 0.4961685729910287\n",
      "Vote= 0.007663004009116041\n",
      "Sum of weights = 1.0035957057650828\n",
      "Starting Classfier number: 85\n",
      "Tree Constrcuted\n",
      "Error= 0.4983860388956789\n",
      "Vote= 0.0032279334198044947\n",
      "Sum of weights = 1.00152017648943\n",
      "Starting Classfier number: 86\n",
      "Tree Constrcuted\n",
      "Error= 0.49903414700176896\n",
      "Vote= 0.0019317083991800925\n",
      "Sum of weights = 1.0009107094868575\n",
      "Starting Classfier number: 87\n",
      "Tree Constrcuted\n",
      "Error= 0.4986893591757523\n",
      "Vote= 0.002621287652231567\n",
      "Sum of weights = 1.001235775768441\n",
      "Starting Classfier number: 88\n",
      "Tree Constrcuted\n",
      "Error= 0.4989450751907056\n",
      "Vote= 0.0021098527492380494\n",
      "Sum of weights = 1.0009948958541761\n",
      "Starting Classfier number: 89\n",
      "Tree Constrcuted\n",
      "Error= 0.49931104950265986\n",
      "Vote= 0.0013779018667140423\n",
      "Sum of weights = 1.000650274826979\n",
      "Starting Classfier number: 90\n",
      "Tree Constrcuted\n",
      "Error= 0.49939532718542184\n",
      "Vote= 0.001209346218719613\n",
      "Sum of weights = 1.0005708560632134\n",
      "Starting Classfier number: 91\n",
      "Tree Constrcuted\n",
      "Error= 0.4988878438740331\n",
      "Vote= 0.0022243159202544375\n",
      "Sum of weights = 1.0010492095741879\n",
      "Starting Classfier number: 92\n",
      "Tree Constrcuted\n",
      "Error= 0.498915169741865\n",
      "Vote= 0.0021696639207856062\n",
      "Sum of weights = 1.0010236068029508\n",
      "Starting Classfier number: 93\n",
      "Tree Constrcuted\n",
      "Error= 0.4991609053853859\n",
      "Vote= 0.0016781908046697247\n",
      "Sum of weights = 1.0007920034564592\n",
      "Starting Classfier number: 94\n",
      "Tree Constrcuted\n",
      "Error= 0.4996761282657485\n",
      "Vote= 0.0006477435590946763\n",
      "Sum of weights = 1.0003058608458097\n",
      "Starting Classfier number: 95\n",
      "Tree Constrcuted\n",
      "Error= 0.5001771027121481\n",
      "Vote= -0.0003542054391092684\n",
      "Sum of weights = 0.999832706657236\n",
      "Starting Classfier number: 96\n",
      "Tree Constrcuted\n",
      "Error= 0.4997354638990263\n",
      "Vote= 0.0005290722513128893\n",
      "Sum of weights = 1.000249758494165\n",
      "Starting Classfier number: 97\n",
      "Tree Constrcuted\n",
      "Error= 0.49968531274523237\n",
      "Vote= 0.0006293745926363714\n",
      "Sum of weights = 1.0002970519918917\n",
      "Starting Classfier number: 98\n",
      "Tree Constrcuted\n",
      "Error= 0.49937875758786215\n",
      "Vote= 0.0012424854636459873\n",
      "Sum of weights = 1.0005867550147003\n",
      "Starting Classfier number: 99\n",
      "Tree Constrcuted\n",
      "Error= 0.4988338063160142\n",
      "Vote= 0.0023323915974050513\n",
      "Sum of weights = 1.0011008089317988\n"
     ]
    }
   ],
   "source": [
    "#making 10 trees using AdaBoosting\n",
    "import math\n",
    "trees = [] #The trees arrays\n",
    "votes = [] #The votes of each tree array\n",
    "\n",
    "num_trees = 100\n",
    "\n",
    "for j in range(0,num_trees):\n",
    "    print(\"Starting Classfier number: \"+str(j))\n",
    "    curr_set = make_data_set(training_data,weights) #Get the dataset based on weights\n",
    "    curr_tree = make_decision_tree(curr_set) #Make decision tree from the dataset\n",
    "    trees.append(curr_tree)\n",
    "    print(\"Tree Constrcuted\")\n",
    "    error = 0\n",
    "    correctness_array = []\n",
    "    for i in range(0,len(training_data)): #Checking predictions for the training set to calculate error\n",
    "        \n",
    "        d = []\n",
    "        for k in range(0,j+1):\n",
    "            temp = predict(training_data[i],trees[k])\n",
    "            d.append(temp)\n",
    "        oo = merge(d)\n",
    "        if (gini_dict(oo)<0.4):\n",
    "             x= predict_result(oo)\n",
    "        else:\n",
    "            x = predict_average(oo) #x is the predicted class for the current tuple\n",
    "        #temp = predict(row,curr_tree) \n",
    "        actual = int(training_data[i][-1])\n",
    "        if (actual!=x):\n",
    "            error = error+ weights[i]*((abs(actual-x)/7)**2)\n",
    "            correctness_array.append(-(abs(actual-x)/7)**2)\n",
    "        else:\n",
    "            correctness_array.append(1)\n",
    "    print(\"Error= \"+str(error))        \n",
    "    curr_vote = 0.5*math.log((1-error)/error) ##Calculating vote for current classifier\n",
    "    votes.append(curr_vote)\n",
    "    print(\"Vote= \"+str(curr_vote))\n",
    "    for i in range(0,len(weights)):\n",
    "        weights[i] = weights[i]*math.exp(-curr_vote*correctness_array[i]) ##Updating weights\n",
    "    sum_weights = sum(weights)\n",
    "    print(\"Sum of weights = \" + str(sum_weights))\n",
    "    weights = [a/sum_weights for a in weights] ##Normalizing weights       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #checking the accuracy for training set(first 100 tuples)\n",
    "# #label_weights = get_label_weights(training_data)\n",
    "# for row in training_data[:100]:\n",
    "#     d = []\n",
    "#     for i in range(0,num_trees):\n",
    "#         temp = predict(row,trees[i])\n",
    "#         #temp = process(temp,votes[i])\n",
    "#         d.append(temp)\n",
    "#     oo = (merge(d))\n",
    "#     #oo = multiply_dicts(oo,label_weights)\n",
    "#     ll = predict_average(oo)\n",
    "#     print(gini_dict(oo))\n",
    "#     if (gini_dict(oo)<0.4):\n",
    "#         ll = predict_result(oo)\n",
    "#     else:\n",
    "#         ll = predict_average(oo)\n",
    "#     print (\"Actual: %s. Predicted: %s and result: %s\" %\n",
    "#            (row[-1], oo,ll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading the test dataset\n",
    "f2 = open(\"cleaned_testing.csv\",\"r\")\n",
    "testing_data = []\n",
    "header = f2.readline()\n",
    "for i in range(0,10000):\n",
    "    temp = f2.readline()\n",
    "    temp = temp.strip().split(',')\n",
    "    temp = temp[1:]\n",
    "    temp = [round(float(a),1) for a in temp]\n",
    "    testing_data.append(temp) \n",
    "len(testing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#label_weights = get_label_weights(training_data)\n",
    "dict_predictions = []\n",
    "for row in testing_data:\n",
    "    d = []\n",
    "    \n",
    "    for i in range(0,num_trees):\n",
    "        temp = predict(row,trees[i])\n",
    "        #temp = process(temp,votes[i])\n",
    "        d.append(temp)\n",
    "    oo = merge(d)\n",
    "    #oo = multiply_dicts(oo,label_weights)\n",
    "    if (gini_dict(oo)<0.4):\n",
    "        ll = predict_result(oo)\n",
    "    else:\n",
    "        ll = predict_average(oo)\n",
    "    dict_predictions.append(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 2,\n",
       " 6,\n",
       " 7,\n",
       " 4,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 8,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 7,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 8,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 8,\n",
       " 6,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 3,\n",
       " 8,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 7,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 8,\n",
       " 4,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 5,\n",
       " 8,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 6,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 7,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 8,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 7,\n",
       " 3,\n",
       " 8,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 7,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 6,\n",
       " 8,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 3,\n",
       " 1,\n",
       " 8,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 7,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 3,\n",
       " 6,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 1,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 8,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 8,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 4,\n",
       " 7,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 7,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 2,\n",
       " 7,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 8,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 7,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 3,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 3,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 1,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 6,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 1,\n",
       " 3,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 4,\n",
       " ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Writing the results        \n",
    "f2 = open(\"out_boosting_10_classifiers_gini_improved_and_updated_average_with_votes.csv\",\"w\")\n",
    "f2.write(\"Id,Response\\n\")\n",
    "for i in range(0,10000):\n",
    "    hh = str(20000+i)+\",\"+str(dict_predictions[i])+\"\\n\"\n",
    "    f2.write(hh) \n",
    "\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
